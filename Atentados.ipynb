{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atentados.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBBigfAr4bh+P3SO3FhAVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marceloraimundo/csv/blob/main/Atentados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Modelizacion de atentados en el gran Buenos Aires durante 1959**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Mi idea fue modelizar una serie de atentados ocurridos durante 1959, año que forma parte del proceso conocido como \"Resistencia Peronista\", en 18 ciudades del gran Buenos Aires.\n",
        "\n",
        "Los datos corresponden a los contenidos en un legajo de la Dirección de Inteligencia de la Policia de Buenos Aires (DIPBA). Para mas información pueden consultar: https://programminghistorian.org/es/lecciones/visualizacion-y-animacion-de-tablas-historicas-con-R\n",
        "\n",
        "*Problema planteado*: Si bien este legajo es bastante completo, cuando se recurre a otra documentación, como los periódicos, surgen diferencias con la información de espioneje, comunmente un subregistro por parte de la policía, es decir en el diario podemos encontrar que se les escapan atentados. Lo que me propuse fue crear un modelo al que pueda ingresarle fechas no registradas y me estime si fue probable un atentado en determinado día, como estrategia para hacer una busqueda dirigida en los periódicos que permita encontrar posibles subregistros por parte de la policía, sin necesidad de revisar toda la colección de periodicos de 1959."
      ],
      "metadata": {
        "id": "WbSLC-zcrBaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Comienzo instalando los distintos paquetes a utilizar\n",
        "\n"
      ],
      "metadata": {
        "id": "JIxfc3wg1Wil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "99MYS4E6Xba_"
      },
      "outputs": [],
      "source": [
        "# Aquí importo los paquetes que voy a usar de Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Scaling y train test split desde SciKit\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Evaluación\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
        "\n",
        "# Manejo de datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Importo datos en formato cvs desde GitHub (que no pide permiso como al usar Gdrive)\n",
        "\n"
      ],
      "metadata": {
        "id": "dWaMbNuT1rae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importo csv con datos desde GitHub\n",
        "at1959=pd.read_csv('https://raw.githubusercontent.com/marceloraimundo/csv/main/Atentadxs%2059.csv')\n"
      ],
      "metadata": {
        "id": "9J77Yr2dA5uN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "El dataframe contiene 3 columnas: Fecha, Ciudad y Atentado\n",
        "\n",
        "Fecha: en formato dd/mm/aaaa, ciudad en string y atentado es 0/1 según si no hubo o hubo algun atentado en dicha ciudad durante esa fecha\n",
        "\n",
        "Nota: *En cantidad de casos es un dataset muy pequeño, pero decidí usarlo ya que encontre en la web casos de modelos de redes neuronales probados sobre pequeños conjunto de datos (estudios sobre diabetes y cancer)*\n",
        "\n"
      ],
      "metadata": {
        "id": "5zfG695O2EMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(at1959)"
      ],
      "metadata": {
        "id": "iwoelJSDKzB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota: *Para realizar un procesamiento adecuado en sintonía con el sentido del seminario de redes neuronales, debí modificar la tabla inicial de datos, ya que el legajo policial incluye sólo positivos (cuando hubo atentado). Lo que resolví para esta ocasión es sumar a los 265 casos positivos, 57 casos repartidos bastante proporcionalemente (en los meses y en las ciudades) que incluyen fechas donde no hubo atentados (o sea atentado=0). Si hubiera sido mejor generar los casos nuevos con algún calculo aleatorio, es una duda que me queda.*"
      ],
      "metadata": {
        "id": "emxwKOon2sQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Como los datos corresponden solo a un año y ademas no se abordaron cuestiones\n",
        "referentes a series temporales (tema que no manejo) tome otra desición: las fechas no las tomo como tales, sino que a partir de ellas genere dos vectores, uno con el número del día y el otro con el número de mes.\n",
        "Para hacerlo elegí la función DatatimeIndex, con la idea de capturar el día y el mes pero no como entero sino como un índice, pensando que con ello evito confundir a la red (los enteros pueden atribuir proximidad, no se si lo resolví correctamente...)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p3qgix4_4-O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hago un vector con días y meses \n",
        "\n",
        "dia = pd.DatetimeIndex(at1959['fecha']).day\n",
        "mes = pd.DatetimeIndex(at1959['fecha']).month\n"
      ],
      "metadata": {
        "id": "qCGL0GovpFMq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Como las ciudades son datos categóricos y la red debe recibir como entrada números, realicé un One Hot Encoding para convertir los valores\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sajN8KvR6znN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hago one hot para la variable categorica\n",
        "ciudades = pd.get_dummies(at1959.ciudad)\n",
        "print(ciudades)\n"
      ],
      "metadata": {
        "id": "xrdF2G_mij6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Luego concatené dia, mes y ciudad en un dataframe llamado at59"
      ],
      "metadata": {
        "id": "I--6DOHy7IyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concateno vectores en dataframe y modifico nombre columnas\n",
        "at59 = pd.concat([pd.DataFrame(dia),pd.DataFrame(mes), ciudades], axis=1)\n",
        "at59.columns.values[0]=\"dia\"\n",
        "at59.columns.values[1]=\"mes\"\n",
        "print(at59)\n",
        "# Guardo el nombre de las columnas para luego poder crear dataframe de datos nuevos 'x_test'\n",
        "at59_columns=list(at59.columns.values)"
      ],
      "metadata": {
        "id": "DIX8a4rFvDd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Posteriormente, con los datos de la columna objetivo que sería Atentado (toma valor 0 si no hubo en esa fecha y lugar, o valor 1 si hubo), creé el vector target59\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F_XXjf5Z7VJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino los resultados\n",
        "target59 = at1959.atentado"
      ],
      "metadata": {
        "id": "alkxmjyQ4hoy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Una vez que termino con la de adaptar los datos para que sean cargables en la red, tomo la libreria train_test_split de SciKit para dividir el conjunto de datos en uno de entrenamiento y otro de test (elegí un 33%)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WC3wxHI8G1jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split del dataframe\n",
        "at59_train, at59_test, target59_train, target59_test = train_test_split(at59,target59,test_size=0.33,random_state=101) "
      ],
      "metadata": {
        "id": "t8c93lPqtBer"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Ahora paso a crear y trabajar con el **modelo de red** (tome uno de los modelos dados en el seminario)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "P_ZrpOqdHpaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo el modelo\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=20, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "TNjo5fcm4JAf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilo el modelo - Salida binaria: hubo o no hubo atentado en determinada fecha y lugar\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "tayZTW015Ipw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entreno el modelo\n",
        "history = model.fit(at59_train, target59_train, validation_data=(at59_test,target59_test), epochs=50, batch_size=10)"
      ],
      "metadata": {
        "id": "bcL31MG-5QJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hago la grafica de pérdida y precisión\n",
        "plt.plot(history.history['loss'], label='MAE (training data)')\n",
        "plt.plot(history.history['val_loss'], label='MAE (validation data)')\n",
        "plt.title('MAE')\n",
        "plt.ylabel('Valor MAE')\n",
        "plt.xlabel('Epoca')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r4zgVD8woSW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Evaluación y predicción** - tomando el mismo conjunto de test\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qi8VP07QI7HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalua el modelo / con el conjunto entero de datos....\n",
        "scores = model.evaluate(at59, target59)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "9720tPoj5dMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones sobre el conjunto de test\n",
        "predictions = model.predict(at59_test)\n",
        "\n",
        "print('MAE: ',mean_absolute_error(target59_test,predictions))\n",
        "print('MSE: ',mean_squared_error(target59_test,predictions))\n",
        "print('RMSE: ',np.sqrt(mean_squared_error(target59_test,predictions)))\n",
        "print('Variance Regression Score: ',explained_variance_score(target59_test,predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "1B6XJpLDgoyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Predicción utilizando otro set de datos**\n",
        "\n",
        "La realizo con un set de datos de otra procedencia, un conjunto de 5 atentados, pero extraidos de otra fuente (Diario El Día)\n",
        "\n",
        "*Nota*: Aquí tuve que codificar estos datos realizando operaciones que me permitan recuperar el total de categorias correspondientes al total de ciudades en juego (son 18), pues si usaba directamente el mismo metodo de mas arriba con un set de test mas pequeño, en este caso 5 atentados en 3 ciudades, construyo un dataframe con menos columnas que las definidas en la capa de entrada de la red (está compilada para recibir 20 columnas y si hacia el get_dummies directo sobre el nuevo set solo genera 5 columnas)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-wIQqzEf5QD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importo csv con datos nuevos para predecir desde GitHub (fuente: Diario El Día - Mayo 1959)\n",
        "at1959_eldia=pd.read_csv('https://raw.githubusercontent.com/marceloraimundo/csv/main/Atentadxs%2059_ElDia.csv')"
      ],
      "metadata": {
        "id": "-idXgrN2NHWn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(at1959_eldia)"
      ],
      "metadata": {
        "id": "DF-00MW7NGqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creo dataframe x_test (que es el que construyo para probar prediccion) tomando nombres de columnas de at59\n",
        "x_test=pd.DataFrame(columns=at59_columns) \n",
        "\n",
        "# Almaceno los resultados para luego chequear las predicciones\n",
        "target59_eldia = at1959_eldia.atentado\n",
        "\n",
        "# Nuevamente hago un vector con días y meses \n",
        "dia_test = pd.DatetimeIndex(at1959_eldia['fecha']).day\n",
        "mes_test = pd.DatetimeIndex(at1959_eldia['fecha']).month\n",
        "\n",
        "# También hago one hot para la variable categorica\n",
        "ciudades_test = pd.get_dummies(at1959_eldia.ciudad)\n",
        "\n",
        "# Me quedo con los nombres de las columnas del dataframe nuevo\n",
        "nom_city=ciudades_test.columns.values\n",
        "\n",
        "# asigno vectores dia y mes al x_test\n",
        "x_test.dia = dia_test\n",
        "x_test.mes = mes_test\n"
      ],
      "metadata": {
        "id": "gzgjTPO8tEDU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aca reemplazo las columnas de ciudades del x_test con las que vienen de ciudades de at1959_eldia\n",
        "for i in range (len(nom_city)):\n",
        "    x_test.loc[:,[nom_city[i]]]=ciudades_test.loc[:,nom_city[i]]\n",
        "\n",
        "# Pongo a 0 las Nan\n",
        "x_test=x_test.fillna(0)\n",
        "\n",
        "# Este es el dataset que puedo mandar a la red\n",
        "at59_eldia_test = x_test\n"
      ],
      "metadata": {
        "id": "IqkhZE2m3ueX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Evaluación y predicción** \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ApBh45JhMVnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluo el modelo / con los datos nuevos de otro origen\n",
        "scores = model.evaluate(at59_eldia_test, target59_eldia)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "tJSaZ-LBI3oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones sobre los datos de otro origen\n",
        "\n",
        "predictions = model.predict(at59_eldia_test)\n",
        "\n",
        "print('MAE: ',mean_absolute_error(target59_eldia,predictions))\n",
        "print('MSE: ',mean_squared_error(target59_eldia,predictions))\n",
        "print('RMSE: ',np.sqrt(mean_squared_error(target59_eldia,predictions)))\n",
        "print('Variance Regression Score: ',explained_variance_score(target59_eldia,predictions))\n",
        "\n",
        "# Resultados Predicción \n",
        "for i in range (len(predictions)):\n",
        "      print ('Caso ', i, '--> Resultado ', np.round(predictions[i]))"
      ],
      "metadata": {
        "id": "ng-rZcQa8Ag3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Conclusión parcial**\n",
        "\n",
        "Pienso que el modelo no es muy bueno, mas allá que hizo predicciones 'acertadas' con la informacion del dataset de origen extra-policial.\n",
        "\n",
        "Si bien no entiendo mucho de estadistica, valores de R2 rondando 0 parece que no habla muy bien del modelo. El *loss* me parece muy alto, aunque se combina con una buena *accuracy*\n",
        "\n",
        "\n",
        "## **Posibles soluciones**\n",
        "\n",
        "Realizar pruebas variando los hiperparámetros (hice algunas, variando cantidad de neuronas, agregando/sacando capas, cambiando epocas y lotes, usando otros optimizers, pero no hubo variaciones notables respecto loss, accuracy, Mae y R2)\n",
        "\n",
        "Aumentar el dataset de entrenamiento y validación usando algun metodo de los que comentaron los docentes en clase (K-Fold)\n",
        "\n",
        "En ambos casos, necesitaría una guia breve por parte de los docentes, pues se me dificulta hacerlo por mi cuenta\n",
        "\n",
        "Desde ya gracias por todo, Marcelo\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "knDXSMxOfr8D"
      }
    }
  ]
}